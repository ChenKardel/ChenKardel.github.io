<!DOCTYPE html>
<html lang="z">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Pix2pix:基于条件对抗网络图对图翻译 | Kardel的魔法领域</title>
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="Pix2pix:基于条件对抗网络图对图翻译我现在正在研究GAN，GAN也正是我的兴趣方向之一。
本篇要讲的是pix2pix，讲道理，那个网页版我可以玩一年
Pix2pix官网
pix2pix论文
Pix2pix的github
（注：原代码是用torch写的，但是作者也在官网提供了tensorflow版和pytorch版。起码不用面对lua这个语言）

图像翻译“翻译”常用于语言之间的翻译，就比如中">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Pix2pix:基于条件对抗网络图对图翻译 | Kardel的魔法领域">
    <meta name="twitter:description" content="Pix2pix:基于条件对抗网络图对图翻译我现在正在研究GAN，GAN也正是我的兴趣方向之一。
本篇要讲的是pix2pix，讲道理，那个网页版我可以玩一年
Pix2pix官网
pix2pix论文
Pix2pix的github
（注：原代码是用torch写的，但是作者也在官网提供了tensorflow版和pytorch版。起码不用面对lua这个语言）

图像翻译“翻译”常用于语言之间的翻译，就比如中">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Pix2pix:基于条件对抗网络图对图翻译 | Kardel的魔法领域">
    <meta property="og:description" content="Pix2pix:基于条件对抗网络图对图翻译我现在正在研究GAN，GAN也正是我的兴趣方向之一。
本篇要讲的是pix2pix，讲道理，那个网页版我可以玩一年
Pix2pix官网
pix2pix论文
Pix2pix的github
（注：原代码是用torch写的，但是作者也在官网提供了tensorflow版和pytorch版。起码不用面对lua这个语言）

图像翻译“翻译”常用于语言之间的翻译，就比如中">

    
    <meta name="author" content="kardel">
    
    <link rel="stylesheet" href="/css/vno.css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css">

    
    <link rel="icon" href="/images/avatar-icon.png">
    

    <meta name="generator" content="hexo"/>
    
    <link rel="alternate" type="application/rss+xml" title="Kardel的魔法领域" href="/atom.xml">
    

    <link rel="canonical" href="http://yoursite.com/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/"/>

                 
</head>

<body class="home-template no-js">
    <script src="//cdn.bootcss.com/jquery/2.1.4/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>

    
<header class="panel-cover panel-cover--collapsed" style="background-image: url(/images/background-cover.jpg)">
  <div class="panel-main">
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 Kardel的魔法领域 的主页"><img src="/images/avatar.jpg" width="80" alt="Kardel的魔法领域 logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage for Kardel的魔法领域">Kardel的魔法领域</a></h1>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description"></p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">
          <div>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="/#blog" title="Visit the blog" class="blog-button">Blog</a></li>
            
              <li class="navigation__item"><a href="/aboutme">关于我</a></li>
            
            </ul>
          </nav>
          </div>
          <div>
          <nav class="cover-navigation navigation--social">
  <ul class="navigation">

  <!-- Weibo-->
  

  <!-- Github -->
  
  <li class="navigation__item">
    <a href="https://github.com/ChenKardel" title="GitHub" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>


<!-- Stack Overflow -->
        

  <!-- Google Plus -->
  

<!-- Facebook -->

  
<!-- Twitter -->

  

  <li class="navigation__item">
    <a href="/atom.xml" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>



  </ul>
</nav>

          </div>
        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-purple"></div>
  </div> 
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single">

  <header class="post-header">
    <div class="post-meta">
      <time datetime="2018-06-02T09:30:06.000Z" class="post-list__meta--date date">2018-06-02</time> &#8226; <span class="post-meta__tags tags">于 
  <a class="tag-link" href="/tags/深度学习/">深度学习</a>, <a class="tag-link" href="/tags/计算机视觉/">计算机视觉</a>, <a class="tag-link" href="/tags/论文解析/">论文解析</a>
 </span>
      <span class="page-pv">
       Read <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>
      </span> 
   
    </div>
    <h1 class="post-title">Pix2pix:基于条件对抗网络图对图翻译</h1>
  </header>

  <section class="post">
    <h1 id="Pix2pix-基于条件对抗网络图对图翻译"><a href="#Pix2pix-基于条件对抗网络图对图翻译" class="headerlink" title="Pix2pix:基于条件对抗网络图对图翻译"></a>Pix2pix:基于条件对抗网络图对图翻译</h1><p>我现在正在研究GAN，GAN也正是我的兴趣方向之一。</p>
<p>本篇要讲的是pix2pix，讲道理，那个网页版我可以玩一年</p>
<p><a href="https://phillipi.github.io/pix2pix/" target="_blank" rel="noopener">Pix2pix官网</a></p>
<p><a href="https://arxiv.org/pdf/1611.07004.pdf" target="_blank" rel="noopener">pix2pix论文</a></p>
<p><a href="https://github.com/phillipi/pix2pix" target="_blank" rel="noopener">Pix2pix的github</a></p>
<p>（注：原代码是用torch写的，但是作者也在官网提供了tensorflow版和pytorch版。起码不用面对lua这个语言）</p>
<p><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/header.bmp" alt="header"></p>
<h2 id="图像翻译"><a href="#图像翻译" class="headerlink" title="图像翻译"></a>图像翻译</h2><p>“翻译”常用于语言之间的翻译，就比如中文和英文的之间的翻译。但是图像翻译的意思是以不同形式在图与图之间转换。比如，一张场景可以转换为RGB全彩图，也可以转化成素描，也可以转化为灰度图。一张夜景图也可以转化为这个地方的日景图。</p>
<p>传统的来说，每一种转换，比如从灰度图到素描，或者从素描到灰度图，都是需要一种特定的算法。而Pix2pix的目标就是建立一个通用的架构去解决所有的这些问题。</p>
<p>自编码器与CNN，为了使loss function更小而不得不使生成出来的图片模糊。这些对loss function有益的行为最终导致的是生成很假的图片（讲个道理，模糊的原因与很多自编码器采取sigmoid有没有关系？笔者觉得使用leaky relu可能会减少模糊，但是笔者试过，效果不算好）GAN提供了一种生成精准图片的方法，原因是识别网络完全能判断出模糊的图像是假图。</p>
<p>这在里，作者使用了条件GAN（cGAN）去学习一个条件生成模型。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>cGAN的损失可以表达为：</p>
<p>$\mathcal{L}<em>{cGAN}(G,D)=\mathbb{E}</em>\mathrm{x,y}[log D(x, y)]+\mathbb{E}_\mathrm{x,z}[log(1-D(x, G(x,z)))]$</p>
<p>可以看出，较普通的GAN来说，cGAN的生成网络监听了输入x。</p>
<p>普通的GAN的形式是</p>
<p>$\mathcal{L}<em>{GAN}(G,D)=\mathbb{E}</em>\mathrm{y}[log D(x, y)]+\mathbb{E}_\mathrm{x,z}[log(1-D(x, G(x,z)))]$</p>
<p>可以明显看出，这个形式的生成网络不需要输入x</p>
<p>研究表明如果给GAN加上一个传统距离（regularization?），比如L2，GAN的表现更好。而Pix2pix使用的是L1架构，可以减少模糊程度</p>
<p>$\mathcal{L}<em>{L1}(G)=\mathbb{E}</em>\mathrm{x,y,z}[\Arrowvert y-G(x,z)\Arrowvert_1]$</p>
<p>所以最终的loss function是</p>
<p>$G^*=argmin_Gmax_D\mathcal{L}<em>{cGAN}(G,D)+\lambda\mathcal{L}</em>{L1}(G)$</p>
<p>原版GAN中的噪音noise，在这里变成了dropout形似的</p>
<p>###网络架构</p>
<h4 id="生成网络：U-net"><a href="#生成网络：U-net" class="headerlink" title="生成网络：U-net"></a>生成网络：U-net</h4><p>由于是个conditional GAN，这个生成者是个自编码器（输入一张图片大小，输出一张图片大小）。但是较传统用自编码器的写法不同，作者使用了U-Net<br><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/Unet.bmp" alt="Unet"></p>
<p>U-Net的论文具体可以看</p>
<p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>
<p>这篇论文（虽然我没看（逃））</p>
<p>U-Net在Autoencoder的基础上，使用了skip-connection。这么做的原因在于很多网络需要讲input的低阶信息分享给output。</p>
<p>skip-connection的特点在于将decoder和encoder连接起来。貌似与ResNet的链接有一定相像，都是相加（不确定，如果有大佬矫正的话请评论）。从上图也可以看到U-net的架构。</p>
<h4 id="识别网络-PatchGAN"><a href="#识别网络-PatchGAN" class="headerlink" title="识别网络: PatchGAN"></a>识别网络: PatchGAN</h4><p>L2 损失比L1损失更加容易产生模糊效益。所以在网络架构上我们会采取L1去解决问题。</p>
<p>因为我们已经可以通过L1获取低频的结构，我们想要获取高频出现的架构。于是我们采取了PatchGAN的架构。PatchGAN只会去判定每一个每一个小的$N*N$的Patch中的结构是真或假并且惩罚它。并且计算平均数作为最终的结果。</p>
<p>这是个马尔科夫过程，也可以理解为风格损失（局部的）</p>
<h4 id="优化和推断"><a href="#优化和推断" class="headerlink" title="优化和推断"></a>优化和推断</h4><p>优化没啥可说的，mini batch SGD</p>
<p>推断注意使用了dropout和batch normalization。training batch 被设为1，这个操作被叫做“instance normalization”</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>一般为了效率我不喜欢看Experiment部分……很多Experiment都是作者自己吹比……但是这个Experiment不知道有多劲爆……</p>
<h4 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h4><p>两种手法</p>
<ol>
<li><p>Amazon  Mechanical Turk（AMT）</p>
<p>这个……感觉事50个Turker去评价算法生成……</p>
<p>感觉这个是真人评价……有点屌……</p>
</li>
<li><p>FCN-score</p>
<p>是采用的语言分段的算法做的评价</p>
</li>
</ol>
<p>一些实验的结论：</p>
<ol>
<li><p>生成网络：U-Net的效果是最好的</p>
</li>
<li><p>识别网络：L1+cGAN的效果比L1/L1+GAN/cGAN/GAN都好</p>
</li>
<li><p>PatchGAN的N：在70的时候表现是最好的</p>
<p><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/patchGAN.bmp" alt="patchGAN"><br>可以看出在$1<em>1$–&gt;$16</em>16$的趋势下图像越来越清晰，并且FCN-score逐渐增高，颜色以越来越相近，到$70<em>70$打到顶峰。但是较$70</em>70$，$286<em>286$的画质非常差而且FCN-score非常低。可能是由于$286</em>286$需要太多参数，训练复杂</p>
</li>
</ol>
<p>顺便提一点，这个网络的目的不是与输入图相同，而是做出一个以假乱真的假图，所以不会与原图经行任何比对。可以说生成图和原图是独立的。</p>
<p>在颜色上，L1会使图像便暗（我的理解是L1的稀疏性会使图像朝变暗的趋势靠拢），而cGAN完全没有这个问题。所以L1+cGAN可以保持生成图片颜色的鲜艳。</p>
<p>最后就是这个网络的结果，非常有意思<br><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/result1.bmp" alt="result1"><br>还有我自己画的“猫”（我知道有点吓人……）<br><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/result2.bmp" alt="result2"><br><img src="/2018/06/02/Pix2pix-基于条件对抗网络图对图翻译/result3.bmp" alt="result3"></p>

  </section>

</article>

<section class="read-more">
           
    
               
            <div class="read-more-item">
                <span class="read-more-item-dim">Newer Post</span>
                <h2 class="post-list__post-title post-title"><a href="/2018/06/02/通过深度学习增强药药反应和药食反应的预测效果/" title="通过深度学习增强药药反应和药食反应的预测效果">通过深度学习增强药药反应和药食反应的预测效果</a></h2>
                <p class="excerpt">
                
                [TOC]
前言还是不要不务正业了，重新回到DDI的怀抱之中。（笑）
这篇是PNAS（美国科学院）的论文，还是有点分量的……虽然我觉得有点水
本篇文章主要讲的是基于深度学习的药物副作用与药物宇食物的作用的预测。
背景药与药(drug-drug interaction，DDI)，药与食物(drug-f
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2018-06-02T09:34:48.000Z" class="post-list__meta--date date">2018-06-02</time> &#8226; <span class="post-list__meta--tags tags">于 
  <a class="tag-link" href="/tags/二十一世纪科学（滑稽）/">二十一世纪科学（滑稽）</a>, <a class="tag-link" href="/tags/深度学习/">深度学习</a>
</span><a class="btn-border-small" href="/2018/06/02/通过深度学习增强药药反应和药食反应的预测效果/">继续阅读</a></div>
                           
            </div>
        
        
               
            <div class="read-more-item">
                <span class="read-more-item-dim">Older Post</span>
                <h2 class="post-list__post-title post-title"><a href="/2018/06/02/DeLiGAN-对于多样性和有限数据的GAN/" title="DeLiGAN: 对于多样性和有限数据的GAN">DeLiGAN: 对于多样性和有限数据的GAN</a></h2>
                <p class="excerpt">
                
                开始遨游CVPR2017，主要是为了学习更多的深度学习网络架构
讲道理计算机视觉的深度学习架构是深度学习各领域比较先进的，比如CNN，自编码器，GAN，注意力机制都是出于CV然后应用于其他领域的。所以即使我不学CV了还是要看CVPR的论文
论文本身
DeLiGAN

讨论一下GAN的缺点:

Mod
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2018-06-02T09:21:32.000Z" class="post-list__meta--date date">2018-06-02</time> &#8226; <span class="post-list__meta--tags tags">于 
  <a class="tag-link" href="/tags/深度学习/">深度学习</a>, <a class="tag-link" href="/tags/计算机视觉/">计算机视觉</a>, <a class="tag-link" href="/tags/论文解析/">论文解析</a>
</span><a class="btn-border-small" href="/2018/06/02/DeLiGAN-对于多样性和有限数据的GAN/">继续阅读</a></div>
                       
            </div>
        
     
   
   
  
</section>

  

            <footer class="footer">
    <span class="footer__copyright">
        &copy; 2018 kardel - 本站点采用 <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>
       
    </span>
    <span class="footer__copyright">
             - 基于 <a href="http://hexo.io">Hexo</a> 搭建，使用 <a href="https://github.com/monniya/hexo-theme-new-vno ">new-vno</a> 主题，由<a href="https://monniya.com ">@Monniya</a> 修改自 <a href="https://github.com/lenbo-ma/hexo-theme-vno" target="_blank">Vno</a>, 原创出自<a href="http://github.com/onevcat/vno" target="_blank">onevcat</a>
         </span>
       
    
  
         <script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
    
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
   
</footer>


        </div>
    </div>

     
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-78918255-1', 'auto');
	ga('send', 'pageview');
</script>

    
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?9cdad07c755fa23f6aced510c6760e87";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
        })();
    </script>



    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    
</body>
</html>
